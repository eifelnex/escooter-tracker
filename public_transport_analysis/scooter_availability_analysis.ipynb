{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-Scooter Availability Analysis\n",
    "\n",
    "This notebook analyzes whether scooter availability (average distance to nearest scooter) correlates with usage rates across cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import BallTree\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.ops import unary_union\n",
    "from adjustText import adjust_text\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "from tueplots import bundles\n",
    "from tueplots.constants.color import rgb\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from city_info import CITY_POPULATIONS, normalize_city, sanitize_city_name\n",
    "from noise_detection.non_ride_detection import filter_known_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load events\nevents = pd.read_parquet('../vehicle_events_export.parquet')\nevents = events[events['city'].notna()].copy()\nevents['city'] = events['city'].astype('category')\n\nprint(f\"Total events: {len(events):,}, Cities: {events['city'].nunique()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build City Demand Hulls\n",
    "\n",
    "Create polygons representing where people look for scooters:\n",
    "1. Take all departure positions (where scooters were picked up)\n",
    "2. Buffer each point by 100m, merge, fill small holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract departure positions per city (min 5000 departures for reliable spatial coverage)\ncity_departures = {}\nfor city in tqdm(events['city'].cat.categories, desc=\"Extracting departures\"):\n    deps = events[(events['city'] == city) & (events['disappeared'] == True)][['lat', 'lon']]\n    if len(deps) >= 5000:\n        city_departures[city] = deps\n\nprint(f\"Cities with sufficient departures: {len(city_departures)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build demand hulls for visualization\nimport folium\nimport pickle\n\nHULL_CHECKPOINT = 'checkpoints/demand_hulls.pkl'\nos.makedirs('checkpoints', exist_ok=True)\n\ndef fill_small_holes(polygon, min_hole_size_m=100):\n    \"\"\"Remove holes smaller than min_hole_size_m (diameter in meters).\"\"\"\n    min_area_m2 = np.pi * (min_hole_size_m / 2) ** 2\n    min_area_deg2 = min_area_m2 / (111000 ** 2)\n    \n    if polygon.geom_type == 'Polygon':\n        large_holes = [interior for interior in polygon.interiors \n                       if Polygon(interior).area >= min_area_deg2]\n        return Polygon(polygon.exterior, large_holes)\n    elif polygon.geom_type == 'MultiPolygon':\n        return unary_union([fill_small_holes(p, min_hole_size_m) for p in polygon.geoms])\n    return polygon\n\ndef create_demand_hull(positions, buffer_meters=100, min_hole_size_m=100):\n    \"\"\"Create demand hull polygon from positions with 100m radius buffer.\"\"\"\n    buffer_deg = buffer_meters / 111000\n    points = [Point(lon, lat).buffer(buffer_deg) for lat, lon in positions]\n    merged = unary_union(points)\n    merged = fill_small_holes(merged, min_hole_size_m)\n    return merged\n\nif os.path.exists(HULL_CHECKPOINT):\n    with open(HULL_CHECKPOINT, 'rb') as f:\n        city_demand_hulls = pickle.load(f)\nelse:\n    city_demand_hulls = {}\n    for city, deps in tqdm(city_departures.items(), desc=\"Building demand hulls\"):\n        positions = deps[['lat', 'lon']].values\n        hull = create_demand_hull(positions, buffer_meters=100, min_hole_size_m=100)\n        city_demand_hulls[city] = hull\n    \n    with open(HULL_CHECKPOINT, 'wb') as f:\n        pickle.dump(city_demand_hulls, f)\n\nprint(f\"Demand hulls: {len(city_demand_hulls)} cities\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create HTML maps for all cities\nos.makedirs('demand_maps', exist_ok=True)\n\ndef create_city_map(city, hull):\n    \"\"\"Create folium map for a city showing demand hull.\"\"\"\n    centroid = hull.centroid\n    m = folium.Map(location=[centroid.y, centroid.x], zoom_start=13)\n    \n    def add_polygon(poly):\n        coords = [(lat, lon) for lon, lat in poly.exterior.coords]\n        folium.Polygon(locations=coords, color='blue', weight=2, fill=True, fill_opacity=0.3).add_to(m)\n        for interior in poly.interiors:\n            hole_coords = [(lat, lon) for lon, lat in interior.coords]\n            folium.Polygon(locations=hole_coords, color='red', weight=3, fill=True, fill_color='red', fill_opacity=0.5).add_to(m)\n    \n    if hull.geom_type == 'Polygon':\n        add_polygon(hull)\n    elif hull.geom_type == 'MultiPolygon':\n        for poly in hull.geoms:\n            add_polygon(poly)\n    return m\n\nfor city, hull in tqdm(city_demand_hulls.items(), desc=\"Creating maps\"):\n    m = create_city_map(city, hull)\n    safe_name = sanitize_city_name(city)\n    m.save(f'demand_maps/{safe_name}.html')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>## Build Availability Model\n",
    "\n",
    "For each scooter, track when and where it was available:\n",
    "- **Intervals**: (start_time, end_time, lat, lon) - new interval when scooter moves >100m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_availability_intervals(events_df, city, min_move_meters=100):\n    \"\"\"Build availability intervals. New interval when scooter moves >100m.\"\"\"\n    city_events = events_df[events_df['city'] == city].sort_values(['vehicle_id', 'timestamp'])\n    intervals = []\n    \n    for vid, veh_events in city_events.groupby('vehicle_id'):\n        current = None\n        for _, e in veh_events.iterrows():\n            if current is None:\n                current = {'vehicle_id': vid, 'start_time': e['timestamp'], 'lat': e['lat'], 'lon': e['lon']}\n            else:\n                dist_m = np.sqrt((e['lat'] - current['lat'])**2 + (e['lon'] - current['lon'])**2) * 111000\n                if dist_m > min_move_meters:\n                    current['end_time'] = e['timestamp']\n                    intervals.append(current)\n                    current = {'vehicle_id': vid, 'start_time': e['timestamp'], 'lat': e['lat'], 'lon': e['lon']}\n        if current:\n            current['end_time'] = veh_events['timestamp'].iloc[-1]\n            intervals.append(current)\n    \n    if not intervals:\n        return pd.DataFrame(columns=['vehicle_id', 'start_time', 'end_time', 'lat', 'lon', 'duration_min'])\n    \n    df = pd.DataFrame(intervals)\n    df['duration_min'] = (df['end_time'] - df['start_time']).dt.total_seconds() / 60\n    return df[df['duration_min'] >= 1]\n\n# Build for all cities\ncity_availability = {}\nfor city in tqdm(city_departures.keys(), desc=\"Building availability intervals\"):\n    city_availability[city] = build_availability_intervals(events, city)\n\nprint(f\"Availability intervals built for {len(city_availability)} cities\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>## Calculate Average Distance to Nearest Scooter\n",
    "\n",
    "Pipeline:\n",
    "1. **Time sampling**: Every hour across the data range\n",
    "2. **For each sample time**: Filter to scooters actually available at that exact moment\n",
    "3. **Build BallTree**: From positions of currently-available scooters only\n",
    "4. **Location sampling**: 5,000 random points from city demand hull\n",
    "5. **Query**: Find nearest available scooter for each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\n\ndef add_noise(points, max_meters=200):\n    \"\"\"Add random noise up to max_meters to each point.\"\"\"\n    n = len(points)\n    angles = np.random.uniform(0, 2*np.pi, n)\n    distances = np.random.uniform(0, max_meters, n)\n    dlat = (distances * np.cos(angles)) / 111000\n    dlon = (distances * np.sin(angles)) / 111000\n    noisy = points.copy()\n    noisy[:, 0] += dlat\n    noisy[:, 1] += dlon\n    return noisy\n\ndef uniform_sample(points, grid_size_m=50):\n    \"\"\"Keep one random point per grid cell for uniform spatial distribution.\"\"\"\n    grid_deg = grid_size_m / 111000\n    cell_x = (points[:, 1] / grid_deg).astype(int)\n    cell_y = (points[:, 0] / grid_deg).astype(int)\n    cell_ids = cell_x * 1000000 + cell_y\n    \n    df = pd.DataFrame({'lat': points[:, 0], 'lon': points[:, 1], 'cell': cell_ids})\n    uniform = df.groupby('cell').apply(lambda x: x.sample(1)).reset_index(drop=True)\n    return uniform[['lat', 'lon']].values\n\n# Pre-compute uniform sample points for each city\ncity_sample_points = {}\nfor city in tqdm(city_availability.keys(), desc=\"Uniform sampling\"):\n    departures_raw = city_departures[city][['lat', 'lon']].values\n    city_sample_points[city] = uniform_sample(departures_raw, grid_size_m=50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def calculate_median_distance(city, sample_interval_hours=1, show_progress=False):\n    \"\"\"\n    Calculate median distance to nearest scooter.\n    Uses uniform sample points + 200m noise for unbiased spatial coverage.\n    \"\"\"\n    if city not in city_availability or city not in city_sample_points:\n        return np.nan\n    \n    intervals = city_availability[city]\n    sample_pts = city_sample_points[city]\n    \n    if len(intervals) == 0 or len(sample_pts) == 0:\n        return np.nan\n    \n    # Pre-convert intervals to numpy for fast filtering\n    start_times = intervals['start_time'].values\n    end_times = intervals['end_time'].values\n    lats = intervals['lat'].values\n    lons = intervals['lon'].values\n    \n    # Get time range\n    min_time, max_time = intervals['start_time'].min(), intervals['end_time'].max()\n    sample_times = pd.date_range(min_time, max_time, freq=f'{sample_interval_hours}h')\n    \n    all_distances = []\n    iterator = tqdm(sample_times, desc=f\"{city}\", leave=False) if show_progress else sample_times\n    for t in iterator:\n        t_np = np.datetime64(t)\n        \n        # Fast numpy filtering\n        mask = (start_times <= t_np) & (end_times > t_np)\n        if mask.sum() < 5:\n            continue\n        \n        # Build BallTree for active scooters\n        coords_rad = np.radians(np.column_stack([lats[mask], lons[mask]]))\n        tree = BallTree(coords_rad, metric='haversine')\n        \n        # Add noise to sample points for each timestep\n        noisy_pts = add_noise(sample_pts, max_meters=200)\n        distances, _ = tree.query(np.radians(noisy_pts), k=1)\n        all_distances.extend(distances.flatten() * 6371000)  # radians to meters\n    \n    return np.median(all_distances) if all_distances else np.nan\n\n# Quick test\nt0 = time.time()\ntest_dist = calculate_median_distance('Stuttgart', show_progress=True)\nprint(f\"\\nStuttgart: {test_dist:.0f}m median distance ({time.time()-t0:.1f}s)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate for all cities with checkpointing\nDISTANCE_CHECKPOINT = 'checkpoints/availability_results.pkl'\n\nif os.path.exists(DISTANCE_CHECKPOINT):\n    with open(DISTANCE_CHECKPOINT, 'rb') as f:\n        existing_results = pickle.load(f)\nelse:\n    existing_results = {}\n\ncities_to_process = [c for c in city_availability.keys() if c not in existing_results]\n\nfor city in tqdm(cities_to_process, desc=\"Cities\"):\n    median_dist = calculate_median_distance(city, show_progress=True)\n    existing_results[city] = median_dist\n    with open(DISTANCE_CHECKPOINT, 'wb') as f:\n        pickle.dump(existing_results, f)\n\navailability_results = [{'city': city, 'median_distance_m': median_dist} \n                        for city, median_dist in existing_results.items()]\navailability_df = pd.DataFrame(availability_results)\n\nprint(availability_df.sort_values('median_distance_m').to_string())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trip Data for Rides per 1k Inhabitants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load trip data\ntrips = pd.read_parquet('pt_comparison_all_trips.parquet')\ntrips = filter_known_issues(trips, normalize_city_func=normalize_city)\n\ntrips['date'] = pd.to_datetime(trips['d_time']).dt.date\nn_days = trips['date'].nunique()\n\ndaily_rides = trips.groupby('city_name').size() / n_days\ndaily_rides = daily_rides.reset_index()\ndaily_rides.columns = ['city', 'mean_daily_rides']\n\ndaily_rides['population'] = daily_rides['city'].map(CITY_POPULATIONS)\ndaily_rides['rides_per_1k'] = (daily_rides['mean_daily_rides'] / daily_rides['population']) * 1000"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate Availability with Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Merge availability with usage data\nmerged = availability_df.merge(daily_rides, on='city', how='inner')\nmerged = merged[merged['rides_per_1k'].notna() & merged['median_distance_m'].notna()]\n\nprint(f\"Cities with both availability and usage data: {len(merged)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plt.rcParams.update(bundles.icml2024(column=\"half\", nrows=1, ncols=1))\n\nfig, ax = plt.subplots()\n\nax.scatter(merged['median_distance_m'], merged['rides_per_1k'], s=50, c=[rgb.tue_blue])\n\n# Regression line\nz = np.polyfit(merged['median_distance_m'], merged['rides_per_1k'], 1)\np = np.poly1d(z)\nx_line = np.linspace(merged['median_distance_m'].min(), merged['median_distance_m'].max(), 100)\nax.plot(x_line, p(x_line), '--', color=rgb.tue_red, label='Trend')\n\nax.set_xlabel('Median Distance to Nearest Scooter (m)')\nax.set_ylabel('Daily Rides per 1,000 Inhabitants')\nax.set_xlim(0, 3000)\n\ntexts = []\nfor _, row in merged.iterrows():\n    texts.append(ax.text(row['median_distance_m'], row['rides_per_1k'], row['city'], fontsize='small'))\n\nadjust_text(texts, ax=ax, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))\n\nplt.savefig(\"availability_vs_usage.pdf\")\nplt.show()\n\n# Correlation statistics\npearson_r = merged['median_distance_m'].corr(merged['rides_per_1k'])\nspearman_rho, spearman_p = stats.spearmanr(merged['median_distance_m'], merged['rides_per_1k'])\n\nprint(f\"Pearson r = {pearson_r:.3f}, Spearman rho = {spearman_rho:.3f} (p = {spearman_p:.4f})\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvsplat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}